{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis done in the `eda.ipynb`, we can come up with the following data transformation which are required.\n",
    "\n",
    "1) Handling NULL values\n",
    "2) Handling Outliers\n",
    "3) Handling categorical data\n",
    "4) Feature Engineering if any\n",
    "5) Dimentionality Reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run eda.ipynb\n",
    "# we can run other notebooks and use it's variables and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./data/train.csv\")\n",
    "test_data = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling NULL values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Training Data:\n",
      "==============================\n",
      "PoolQC          1453\n",
      "MiscFeature     1406\n",
      "Alley           1369\n",
      "Fence           1179\n",
      "MasVnrType       872\n",
      "FireplaceQu      690\n",
      "LotFrontage      259\n",
      "GarageType        81\n",
      "GarageYrBlt       81\n",
      "GarageFinish      81\n",
      "GarageQual        81\n",
      "GarageCond        81\n",
      "BsmtFinType2      38\n",
      "BsmtExposure      38\n",
      "BsmtFinType1      37\n",
      "BsmtCond          37\n",
      "BsmtQual          37\n",
      "MasVnrArea         8\n",
      "Electrical         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For the records which are having significant NULL values, I am going to drop those columns\n",
    "# getting null value counts\n",
    "null_counts = train_data.isnull().sum()\n",
    "\n",
    "print(\"Null Values in Training Data:\")\n",
    "print(\"=\"*30)\n",
    "sorted_null_counts = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "print(sorted_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the rest of the data I am going to impute the numerical data with mean and categorical data with the mode\n",
    "def handle_null(data):\n",
    "    data.drop(columns = ['PoolQC', 'Alley', 'MiscFeature', 'Fence', 'MasVnrType', 'FireplaceQu'], inplace = True, axis = 1)\n",
    "    \n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == 'float64' or data[column].dtype == 'int64':\n",
    "            data[column].fillna(data[column].mean(), inplace=True)\n",
    "        elif data[column].dtype == 'object':\n",
    "            data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cp = train_data.copy()\n",
    "train_data_clean = handle_null(train_data_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in Training Data:\n",
      "==============================\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "null_counts = train_data_clean.isnull().sum()\n",
    "\n",
    "print(\"Null Values in Training Data:\")\n",
    "print(\"=\"*30)\n",
    "sorted_null_counts = null_counts[null_counts > 0].sort_values(ascending=False)\n",
    "print(sorted_null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cp = test_data.copy()\n",
    "test_data_clean = handle_null(test_data_cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list_train = ['SalePrice', 'LotFrontage', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "out_list_test = ['LotFrontage', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "# using zscore to remove outiers\n",
    "# the standard score is the number of standard deviations by which the value of a raw score is above or below the mean value of what is being observed or measured.\n",
    "train_data_clean = train_data_clean[((np.abs(stats.zscore(train_data_clean[out_list_train])) < 3)).all(axis=1)]\n",
    "test_data_clean = test_data_clean[((np.abs(stats.zscore(test_data_clean[out_list_test])) < 3)).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Handling skewed data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_skew_data(df, skew_threshold=1):\n",
    "    numerical_features = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    skewness = numerical_features.apply(lambda x: stats.skew(x.dropna()))\n",
    "    skewed_features = skewness[skewness > skew_threshold].index\n",
    "    df_transformed = df.copy()\n",
    "    df_transformed[skewed_features] = np.log1p(df[skewed_features])\n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_clean = handle_skew_data(train_data_clean)\n",
    "test_data_clean = handle_skew_data(test_data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only few features based on the correlation matrix we have got\n",
    "wanted_feat = ['OverallQual', 'GrLivArea', '2ndFlrSF', 'GarageCars', 'TotalBsmtSF', 'GarageArea', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'BsmtFinSF1', 'SalePrice']\n",
    "unwanted_feat = list(set(train_data_clean.select_dtypes(include=[\"float64\", \"int64\"]).columns.to_list()) - set(wanted_feat))\n",
    "train_data_clean.drop(columns= unwanted_feat, axis=1, inplace=True)\n",
    "test_data_clean.drop(columns= unwanted_feat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding - for xgboost and decision trees\n",
    "def label_encode_categorical(data):\n",
    "    data_encoded = data.copy()\n",
    "\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object':\n",
    "            data_encoded[col] = LabelEncoder().fit_transform(data[col])\n",
    "\n",
    "    return data_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label_encoded = label_encode_categorical(train_data_clean)\n",
    "test_data_label_encoded = label_encode_categorical(test_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding - for linear\n",
    "def one_hot_encode_categorical(data):\n",
    "    print('before get_dummies() shape:', data.shape)\n",
    "    df_ohe = pd.get_dummies(data)\n",
    "    print('after get_dummies() shape:', df_ohe.shape)\n",
    "    pd.set_option(\"display.max_columns\",300)\n",
    "    df_ohe.reset_index(drop = True)\n",
    "    return df_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before get_dummies() shape: (1407, 51)\n",
      "after get_dummies() shape: (1407, 242)\n",
      "before get_dummies() shape: (1413, 50)\n",
      "after get_dummies() shape: (1413, 226)\n"
     ]
    }
   ],
   "source": [
    "train_data_one_hot_encoded = one_hot_encode_categorical(train_data_clean)\n",
    "test_data_one_hot_encoded = one_hot_encode_categorical(test_data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_label_encoded.to_csv(\"./data/label_encoded_train.csv\", index=False)\n",
    "test_data_label_encoded.to_csv(\"./data/label_encoded_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_one_hot_encoded.to_csv(\"./data/onehot_encoded_train.csv\", index=False)\n",
    "test_data_one_hot_encoded.to_csv(\"./data/onehot_encoded_test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
